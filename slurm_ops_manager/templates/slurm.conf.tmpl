ClusterName={{ cluster_name }}

# active controller
ControlMachine={{ active_controller_hostname }}
ControlAddr={{ active_controller_ingress_address }}

{% if backup_controller_hostname %}
# backup controller
BackupController={{ backup_controller_hostname }}
BackupAddr={{ backup_controller_ingress_address }}
{% elif backup_controller_ingress_address %}
BackupController={{ backup_controller_ingress_address }}
BackupAddr={{ backup_controller_ingress_address }}
{% endif %}


SlurmctldParameters={{ slurmctld_parameters }}

AuthType=auth/munge
AuthAltTypes=auth/jwt
AuthInfo="socket={{ munge_socket }}"
AuthAltParameters="jwt_key={{ jwt_rsa_key_file }}"


CryptoType=crypto/munge

MailProg={{ mail_prog }}

SlurmUser={{ slurm_user }}
SlurmdUser={{ slurmd_user }}

SlurmctldPort=6817
SlurmdPort=6818

SlurmctldPidFile={{ slurmctld_pid_file }}
SlurmdPidFile={{ slurmd_pid_file }}

SlurmctldLogFile={{ slurmctld_log_file }}
SlurmdLogFile={{ slurmd_log_file }}

SlurmdSpoolDir={{ slurm_spool_dir }}
StateSaveLocation={{ slurm_state_dir }}

PluginDir={{ slurm_plugin_dir }}

PlugStackConfig={{ slurm_plugstack_conf }}

ProctrackType={{ proctrack_type }}

#SlurmdDebug={{ log_level }}
#SlurmctldDebug={{ log_level }}

{% if acct_gather_profile %}
AcctGatherProfileType={{ acct_gather_profile }}
{% else %}
AcctGatherProfileType=acct_gather_profile/none
{% endif %}

RebootProgram="/usr/sbin/reboot --reboot"

# User specified config
{{custom_config}}

{% if elasticsearch_address  %}
# Job completion elasticsearch configuration
JobCompType=jobcomp/elasticsearch
JobCompLoc={{elasticsearch_address}}
JobCompParams=timeout=5
{% endif %}

{% if prolog_epilog %}
{% if prolog_epilog.slurmctld_epilog_path %}
EpilogSlurmctld={{prolog_epilog.slurmctld_epilog_path}}
{% endif %}
{% if prolog_epilog.slurmctld_prolog_path %}
PrologSlurmctld={{prolog_epilog.slurmctld_prolog_path}}
{% endif %}
{% endif %}

{% if nhc %}
HealthCheckProgram={{nhc.nhc_bin}}
HealthCheckInterval={{nhc.health_check_interval}}
HealthCheckNodeState={{nhc.health_check_node_state}}
{% endif %}

# SLURMDBD CONFIGURED ON {{ slurmdbd_hostname }}
AccountingStorageType=accounting_storage/slurmdbd
JobAcctGatherType=jobacct_gather/linux
AccountingStorageHost={{ active_slurmdbd_hostname }}
AccountingStoragePort={{ active_slurmdbd_port }}
AccountingStoragePass={{ munge_socket }}
{% if backup_slurmdbd_hostname %}
AccountingStorageBackupHost={{ backup_slurmdbd_hostname }}
{% endif %}

# INCLUDE CLUSTER SPECIFIC CONFIGURATION OVERRIDE
#include /etc/slurm/slurm-%c.conf

# GRES type
GresTypes=gpu

# Enumerate nodes and partitions
{% for partition in partitions %}
{% for node in partition.inventory %}
NodeName={{node.node_name}} NodeAddr={{node.node_addr}} State={{node.state}} RealMemory={{node.real_memory}} CPUs={{node.cpus}} ThreadsPerCore={{node.threads_per_core}} CoresPerSocket={{node.cores_per_socket}} SocketsPerBoard={{node.sockets_per_board}} {{ "Gres=%s" % node.gres if node.gres else ""}}
{%- endfor %}

PartitionName={{partition.partition_name}} Nodes={% for slurmd in partition.inventory %}{{slurmd.node_name}}{{ "," if not loop.last}}{% endfor %} Default={{ 'YES' if partition.partition_default else 'NO' }} State={{partition.partition_state}} {{partition.partition_config if partition.partition_config else ""}}
{% endfor %}

# Enumerate nodes in DownNodes
{% for node in down_nodes %}
DownNodes={{node}} Reason="New node"
{% endfor %}
